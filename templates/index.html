{% extends "layout.html" %}

{% block body %}
<div class="container" width="800px">
    <div class="col-md-10 col-md-offset-1">
        <div class="row" id="page-heading">
            <div class="col-md-12">
                <h1>Flu Forecaster</h1>
                <p>by <a href="https://www.linkedin.com/in/ericmjl/">Eric J. Ma</a>, Insight Health Data Science Fellow, Boston Summer Session 2017</p>
            </div>
        </div>
        <div class="row" id="introduction-text-row">
            <div class="col-md-8" id="introduction-text">
                <h2>Influenza Vaccine Development</h2>
                <p>Influenza vaccine strain selection lags behind deployment by about 6 months to 2 years, while production lags behind deployment by about 6 months. Additionally, vaccine strains are selected from amongst currently-circulating strains.</p>

                <p><b>This means that the flu virus will have evolved by the time the vaccine strain is scaled into production.</b></p>
            </div>
            <div class="col-md-4" id="fig-vaccine">
                <img src="https://raw.githubusercontent.com/ericmjl/flu-sequence-predictor/master/images/vaccine-production.jpg" alt="Vaccine production lag time." width="350px">
                <p><i>Large lag time from vaccine strain selection to actual deployment.</i></p>
            </div>
        </div>
        <div class="row" id="introduction-text-row-2">
            <div class="col-md-8" id="">
                <p>There is a lost business opportunity here, as studies by the CDC are showing that <a href="https://www.cdc.gov/mmwr/volumes/66/wr/mm6606a3.htm?s_cid=mm6606a3_w">the 2017 vaccine efficacy is below 50%</a> on most years, with some years dropping to below 20% efficacy. Sustained lack of efficacy may result in a lack of public confidence in vaccination. The most immediate impact would be a contraction in the market for vaccines, followed by adverse impacts for public health. With a <a href="http://www.cnbc.com/2015/10/19/the-16-billion-business-of-flu.html">USD 4 billion dollars market size globally</a>, vaccine manufacturers cannot afford for public confidence to be eroded.</p>

                <p><b>Good science can help make vaccines that better match what will actually be circulating, which is good business sense, and that is the goal of this project.</b></p>
            </div>
            <div class="col-md-4">
                {{ ve_div|safe }}
                <p><i>Vaccine effectiveness has not historically been high. Data from the <a href="https://www.cdc.gov/flu/professionals/vaccination/effectiveness-studies.htm">CDC</a></i>.</p>
            </div>
        </div>
        <div class="row" id="data-text-row">
            <div class="col-md-8" id="data-source-text">
                <h2>Data</h2>
                <p>The data are sourced from the <a href="https://www.fludb.org/brc/home.spg?decorator=influenza">Influenza Research Database</a>, a publicly-hosted repository of influenza sequencing data. The data I collected span the years {{ nseq_metadata['min_year'] }}-{{  nseq_metadata['max_year'] }}. I only downloaded hemagglutinin (HA) protein sequence, because it is the protein most determinant of an immune response; <b>if the vaccine strain's HA sequence is similar to the circulating strain's HA sequence, then it will provide protection against the circulating strain</b>.</p>

                <p>There are multiple subtypes of influenza; some you may have heard include "H1N1", "H3N2" and "H5N1". I focused only on H3 hemagglutinin protein sequences sourced from human patients worldwide. In total, there are {{ nseq_metadata['n_seqs'] }} influenza sequences considered in the dataset.</p>
            </div>
            <div class="col-md-4" id="data-per-year-plot">
                {{ nseq_div|safe }}
                <p><i>Number of influenza sequences per year in the dataset used for Flu Forecaster.</i></p>
            </div>
        </div>
        <div class="row" id="analysis-methods-and-tools-header-row">
            <div class="col-md-12" id="analysis-methods-and-tools-header-container">
                <h2>Analysis Methods and Tools</h2>
                <p>The key challenge here is finding a representation of "sequences" for which there is a natural notion of "momentum" and "forward" steps. To do this, we use a combination of <b>discretized</b> inputs, variational autoencoders, and gaussian process (GP) regression.</p>

                <p>Click on the buttons below to view more details about the data processing and machine learning methods used.</p>
            </div>
        </div>
        <div class="row" id="analysis-methods-and-tools-text-row">
            <div class="col-md-4" id="preprocessing-column">
                <h3>Preprocessing</h3>
                <p>Sequences were padded with <code>*</code> (asterisk) characters to the maximal length. They were then one-hot-encoded at each position, yielding a sparse binary matrix representation of each protein's amino acid sequence.</p>

                <button type="button" name="learn-more-preprocessing" class="btn btn-primary" data-toggle="modal" data-target="#preprocessingModal">Learn More</button>

                <div class="modal fade" id="preprocessingModal" tabindex="-1" role="dialog" aria-labelledby="preprocessing-modal-title">
                    <div class="modal-dialog" role="document">
                        <div class="modal-content">
                            <div class="modal-header">
                                <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                                <h3 class="modal-title" id="preprocessing-modal-title"> Preprocessing</h3>
                            </div>
                            <div class="modal-body" id="modal-body-preprocessing">
                                <img class="img-responsive center-block" src="https://raw.githubusercontent.com/ericmjl/flu-sequence-predictor/master/images/preprocessing-figures.jpg" alt="" width=80%>
                                <h4>Sequence Padding</h4>
                                <p>Contrary to what might be common thought, multiple sequence alignment was not done prior to conversion to a numerical form. Rather, I first computed the maximum sequence length represented inside the set of sequences downloaded, and padded <code>*</code> (asterisk) characters on each sequence until all of them fit that length. </p>

                                <p>This done using custom Python code and the <code>BioPython</code> and <code>numpy</code> packages.</p>

                                <h4>One-Hot Encoding</h4>
                                <p>Sequences were then one-hot encoded at each position. Memory was not an issue, and as such each amino acid position was converted into 24-long vector. Each vector slot represents an amino acid letter. A "1" is placed in a slot if the amino acid at the currently considered position corresponds to that slot's letter, and a "0" is placed everywhere else. </p>

                                <p>This was accomplished using the <code>sklearn.preprocessing</code> module.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="col-md-4" id="vae-column">
                <h3>Variational Autoencoders</h3>
                <p>VAEs were used to learn a continuous latent embedding/representation of the binary encoded protein sequence on which a Gaussian Process regression model could be used for forecasting. The encoder network compresses the data to the latent embedding, and the decoder network reconstructs sequences from latent embedding coordinates.</p>

                <button type="button" name="learn-more-vae" class="btn btn-primary" data-toggle="modal" data-target="#vaeModal">Learn More</button>

                <div class="modal fade" id="vaeModal" tabindex="-1" role="dialog" aria-labelledby="vae-modal-title">
                    <div class="modal-dialog" role="document">
                        <div class="modal-content">
                            <div class="modal-header">
                                <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                                <h3 class="modal-title" id="vae-modal-title">Variational Autoencoders</h3>
                            </div>
                            <div class="modal-body" id="modal-body-vae">
                                <img class="img-responsive center-block" src="https://raw.githubusercontent.com/ericmjl/flu-sequence-predictor/master/images/vae-icon.jpg" alt="" width="200px">
                                <p>Autoencoders reconstruct the input as the output, and simultaneously learn a lower-dimensional "latent" representation of the original data. With sequences, this has the advantage of converting a "discrete" input into a "continuous" one; with continuous representations, we have a wealth of regression methods that can be used that would not otherwise be available using protein sequences.</p>

                                <p>"Variational" autoencoders learn not just the "point estimate" in "latent embedding space", but also the probability distribution in "latent embedding space" that corresponds to a particular sequence. This means that we can also sample new sequences out of the latent space, and generate new, unseen sequences. This gives us hope for forecasting what new sequences will look like.</p>

                                <p>Here, after converting the sequences to a binary encoding, I passed the binary-encoded matrix through a VAE to learn a 3D embedding, which you can visualize below.</p>

                                <p><b>Basically</b>, VAEs give us a convenient representation of sequence space that lets us do other machine learning on it.</p>

                                <p>Variational autoencoders were implemented using the <code>Keras</code> package.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="col-md-4" id="gp-column">
                <h3>Gaussian Process Regression</h3>
                <p>Gaussian Processes were used to forecast the latent embedding coordinates for future influenza sequences. The forecasted coordinates were passed back to the decoder portion of the VAE to provide the probability distribution over possible sequences.</p>

                <button type="button" name="learn-more-gp" class="btn btn-primary" data-toggle="modal" data-target="#gpModal">Learn More</button>

                <div class="modal fade" id="gpModal" tabindex="-1" role="dialog" aria-labelledby="gp-modal-title">
                    <div class="modal-dialog" role="document">
                        <div class="modal-content">
                            <div class="modal-header">
                                <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                                <h3 class="modal-title" id="gp-modal-title">Gaussian Process Regression</h3>
                            </div>
                            <div class="modal-body" id="modal-body-gp">
                                <!-- <h4>Gaussian Processes (GPs)</h4> -->
                                <img class="img-responsive center-block" width="300px" src="https://raw.githubusercontent.com/ericmjl/flu-sequence-predictor/master/images/gp-icon.jpg" alt="">
                                <p>Gaussian Processes are a "lazy" and Bayesian machine learning method well suited to learning non-linear functions of data. The basic idea behind GPs is as such: given a set of data points sampled from a non-linear function, it will return a probability distribution over the possible functions that fit the dataset. GPs are called "lazy" because they assume that points nearby one another on one axis will be nearby one another on the other axis.</p>

                                <p>In time-series land, GPs are particularly useful because time-series data may be varying in a non-linear fashion with time (think stock markets), and they are able to provide quantified estimates of <b>uncertainty</b> in predictions (because they're Bayesian - Go Bayes!).</p>

                                <p><b>Basically</b>, GPs provide a way of quantifying the uncertainty surrounding predicted sequences.</p>

                                <p>GPs here were implemented using <code>PyMC3</code>.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="row" id="predicted-sequences-row">
            <div class="col-md-12">
                <h2>Predicted Sequences</h2>
                <p>Given the current evolutionary trajectory of the influenza A virus, there are {{ n_seqs }} distinct predicted "average" influenza hemagglutinin sequences for the coming quarter.</p>
            </div>
        </div>
        <div class="row" id="multiple-sequence-alignment-row">
            <div class="col-md-12" id="multiple-sequence-alignment">
                <!-- Do not delete this set of divs. The relevant script is below to make the page load faster. -->
            </div>
        </div>
        <div class="row" id="evoluiontary-trajectory-text-row">
            <div class="col-md-12" id="evolutionary-trajectory-text">
                <h2>Evolutionary Trajectory</h2>
                <p>
                    Using a variational autoencoder, we can visualize the evolutionary trajectory of viral proteins in lower dimensions. Each point in the scatterplot below is a representation of the "average" flu sequence per calendar quarter. The points are coloured by time - dark colours are earlier, and light colours are later.
                </p>
            </div>
        </div>
        <div class="row" id="evolutionary-trajectory-plots-row">
            <div class="col-md-12 center-block" id="evolutionary-trajectory-plot">
                {{ evo_div|indent(4)|safe }}
            </div>
        </div>
        <div class="row" id="improvements-text-row">
            <div class="col-md-12">
                <h2>Improvements</h2>
                <p>Data-driven science is never really done. Here's how I think this project can be taken to the next level.</p>
            </div>
        </div>
        <div class="row" id="improvements-explanation-row">
            <div class="col-md-4" id="gp-training-text">
                <h3>GP Training</h3>
                <p>Right now, we are making predictions on the forecasted sequences one quarter out, and are only predicting the "average sequence" that we expect to see. This "average" sequence actually differs from actually-circulating viruses by anywhere from 6-19 amino acids. Given better compute resources and algorithm tweaking, I would be able to train a GP on the evolutionary coordinates better.</p>

                <p>Apart from this, the GP coordinate dimensions are trained independently. It would be better to train the GP on all 3 dimensions jointly. However, this would also require greater compute resources.</p>
            </div>
            <div class="col-md-4" id="forecasting-validation">
                <h3>Forecasting Validation</h3>
                <p>More work needs to be done on forecasting validation, more specifically, on <b>backtesting</b>. Currently, I am doing model validation by checking that the 2017 "average" coordinates fall within the 95% HPD of the latent space coordinates. I have yet to perform more periods of backtesting, in which I hold out more than just two quarters of data.</p>

                <p>For more information on the best way to perform backtesting, see <a href="http://multithreaded.stitchfix.com/blog/2017/02/28/whats-wrong-with-my-time-series/">this</a> blog post on the StitchFix blog.</p>
            </div>
        </div>
        <div class="row" id="conclusions-row">
            <div class="col-md-12">
                <h2>Conclusions</h2>
                <p>Forecasting how sequences evolve is a tough problem, primarily because there's no notion of "forward momentum" when talking about changes in sequence land. However, if we transform the problem into one involving a "continuous" representation, we can take advantage of concepts like momentum vectors and gradients, and harness tools from continuous time-series regression.</p>

                <p>In this project:</p>
                <ul>
                    <li>VAEs transform discrete sequence to a continuous representation.</li>
                    <li>We use GPs for time-series regression to forecast future continuous space representations of flu.</li>
                    <li>We then take the forecasted representations, and pass them back through the VAE to get back predicted sequences.</li>
                </ul>
            </div>
        </div>
    </div>

</div>

<!-- JS Scripts -->
    <!-- Multiple sequence alignment scripts -->
    <script src="https://s3.eu-central-1.amazonaws.com/cdn.bio.sh/msa/latest/msa.min.gz.js"></script>
    <script>
        var opts = {
            el: document.getElementById("multiple-sequence-alignment"),
            vis: {
              conserv: true,
              overviewbox: false,
              seqlogo: true,
            },
            colorscheme: {
              scheme: "clustal2"
            },
            // smaller menu for JSBin
            // menu: "small",
            bootstrapMenu: true,
            importURL: "https://raw.githubusercontent.com/ericmjl/flu-sequence-predictor/master/data/twoQ_predictions.fasta",
        };

        var m = new msa.msa(opts);
        	m.render();

    </script>

    <!-- Bokeh JS -->
    {{ js_resources|indent(4)|safe }}
    {{ css_resources|indent(4)|safe }}
    <!-- Bokeh Plots -->
    {{ evo_script|indent(4)|safe }}
    {{ nseq_script|safe }}
    {{ ve_script|safe }}

{% endblock %}
